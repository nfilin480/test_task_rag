{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3ef6ae4b7042aba62cf097c1c238cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "import os\n",
    "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "# Загрузка модели (например, Gemma 9B)\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''' You are an expert in natural language processing and query classification. Your task is to label each given query as either \"retrieval needed\" (1) or \"retrieval not needed\" (0). \n",
    "\n",
    "Instructions:\n",
    "1. Analyze the query carefully.\n",
    "2. If the query requires specific, detailed, or specialized external information (e.g., fact-checking, detailed data, or precise terminology), label it as \"1\" (retrieval needed).\n",
    "3. If the query is generic, abstract, or related to tasks like summarization, paraphrasing, or general knowledge that can be answered by a language model without external data, label it as \"0\" (retrieval not needed).\n",
    "4. Provide your answer in the following format: \n",
    "   <label><0 or 1></label>\n",
    "   <explanation><one or two sentences explaining your reasoning></explanation>\n",
    "\n",
    "Examples:\n",
    "Query: When did Virgin Australia start operating?\n",
    "<label>1</label>\n",
    "<explanation>This query asks for a specific historical fact (a start date) that is unlikely to be deduced purely from context and requires external factual data.</explanation>\n",
    "\n",
    "Query: Which is a species of fish? Tope or Rope\n",
    "<label>1</label>\n",
    "<explanation>The query requires knowing the correct fish species between two options, which is a precise factual lookup rather than a generic or inferable answer.</explanation>\n",
    "\n",
    "Query: Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
    "<label>0</label>\n",
    "<explanation>This is a riddle where the answer is directly given in the query (\"Alice\"), making external retrieval unnecessary.</explanation>\n",
    "\n",
    "Query: What individual has won the most Olympic gold medals in the history of the games?\n",
    "<label>1</label>\n",
    "<explanation>This is a specific fact-based question that requires up-to-date and precise information about Olympic records, which typically necessitates an external data source.</explanation>\n",
    "\n",
    "Query: Which Dutch artist painted “Girl with a Pearl Earring”?\n",
    "<label>0</label>\n",
    "<explanation>This is a well-known art fact that most language models can answer from general knowledge without needing to retrieve external information.</explanation>\n",
    "   \n",
    "Now, please label the following query:\n",
    "\n",
    "{query}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для разметки\n",
    "import re \n",
    "\n",
    "def label_query_batch(queries, batch_size=8):\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch_queries = queries[i:i + batch_size]\n",
    "        \n",
    "        # Prepare batch inputs\n",
    "        batch_inputs = []\n",
    "        for query in batch_queries:\n",
    "            input_text = prompt.format(query=query)\n",
    "            messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "            input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            batch_inputs.append(input_text)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(batch_inputs, add_special_tokens=False, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate for batch\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=45,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # Process batch outputs\n",
    "        for output in outputs:\n",
    "            response = tokenizer.decode(output)\n",
    "            try:\n",
    "                # Находим часть ответа после последнего слова \"model\"\n",
    "                parts = re.split(r'<start_of_turn> *model', response, flags=re.IGNORECASE)\n",
    "                response = parts[-1] if len(parts) > 1 else response\n",
    "                #print(response)\n",
    "                \n",
    "                # Ищем метку в формате <label>X</label>\n",
    "                label_match = re.search(r'<label>(\\d)</label>', response)\n",
    "                # Ищем объяснение в формате <explanation>text</explanation>\n",
    "                explanation_match = re.search(r'<explanation>(.*?)</explanation>', response)\n",
    "                \n",
    "                label = int(label_match.group(1)) if label_match else -1\n",
    "                explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n",
    "                \n",
    "                # Поскольку confidence не используется в новом формате, установим его в 1.0\n",
    "                confidence = 1.0 if label != -1 else 0.0\n",
    "            except:\n",
    "                label, confidence, explanation = -1, 0.0, \"\"\n",
    "            \n",
    "            results.append((label, confidence, explanation))\n",
    "    \n",
    "        # Clear CUDA cache after processing each batch\n",
    "        torch.cuda.empty_cache()\n",
    "        # Clear variables\n",
    "        del inputs, outputs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]c:\\Users\\sante\\miniconda3\\envs\\ai2\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "100%|██████████| 1500/1500 [1:10:57<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Примеры предсказаний:\n",
      "                                               query  \\\n",
      "0  Can you recommend a fun DIY project that can b...   \n",
      "1  Hello Assistant! Could you help me out in maki...   \n",
      "2  What are the differences between the Lindy hop...   \n",
      "3  Could you describe the easiest way to draw a c...   \n",
      "4  I want you to act as a Linux terminal. I will ...   \n",
      "\n",
      "                                         instruction  predicted_label  \\\n",
      "0  Can you recommend a fun DIY project that can b...                0   \n",
      "1  Hello Assistant! Could you help me out in maki...                1   \n",
      "2  What are the differences between the Lindy hop...                1   \n",
      "3  Could you describe the easiest way to draw a c...                0   \n",
      "4  I want you to act as a Linux terminal. I will ...               -1   \n",
      "\n",
      "   confidence                                          reasoning  \n",
      "0         1.0  This query seeks a suggestion or recommendatio...  \n",
      "1         1.0                                                     \n",
      "2         1.0                                                     \n",
      "3         1.0  This query asks for a general description of a...  \n",
      "4         0.0                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "def evaluate_model(df, batch_size=8):\n",
    "    #queries = [str(i) + ' '+ str(c) for i, c in zip(df['instruction'].astype(str).tolist(), df['input'].astype(str).tolist())]\n",
    "    queries = df['prompt'].astype(str).tolist()\n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(queries), batch_size)):\n",
    "        batch_queries = queries[i:i + batch_size]\n",
    "        batch_results = label_query_batch(batch_queries, batch_size)\n",
    "        \n",
    "        # Modified this section to include instruction and context\n",
    "        for j, ((pred_label, confidence, reasoning), query) in enumerate(zip(batch_results, batch_queries)):\n",
    "            idx = i + j\n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'instruction': df.iloc[idx]['prompt'],\n",
    "                #'context': df.iloc[idx]['input'],\n",
    "                'predicted_label': pred_label,\n",
    "                'confidence': confidence,\n",
    "                'reasoning': reasoning\n",
    "            })\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            pd.DataFrame(results).to_csv(f'gemma_oasst_{i}.csv', index=False)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "#dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "df = pd.read_csv('./datasets/oasst_suggest.csv')\n",
    "\n",
    "#df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Calculate character length and filter\n",
    "df['total_chars'] = df['prompt'].fillna('').str.len()# + df['input'].fillna('').str.len()\n",
    "df = df[df['total_chars'] <= 2000]\n",
    "#df.to_csv('dolly_15k.csv', index=False)\n",
    "\n",
    "# Запускаем оценку\n",
    "results_df = evaluate_model(df, 2)\n",
    "\n",
    "# Выводим первые несколько результатов\n",
    "print(\"\\nПримеры предсказаний:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_df).to_csv(f'gemma_alpaca_{3000}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       " 0    2013\n",
       " 1     986\n",
       "-1       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['predicted_label'].value_counts()\n",
    "# for i in range(len(results_df[:100])):\n",
    "#     print(results_df['query'][i])\n",
    "#     print(results_df['predicted_label'][i])\n",
    "#     print(results_df['confidence'][i])\n",
    "#     print(results_df['reasoning'][i])\n",
    "#     print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('gemma_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'true_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sante\\miniconda3\\envs\\ai2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'true_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#вывести все ошибки\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m errors \u001b[38;5;241m=\u001b[39m results_df[\u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m errors\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m true_label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, pred_label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, reasoning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sante\\miniconda3\\envs\\ai2\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\sante\\miniconda3\\envs\\ai2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'true_label'"
     ]
    }
   ],
   "source": [
    "#вывести все ошибки\n",
    "errors = results_df[results_df['true_label'] != results_df['predicted_label']]\n",
    "\n",
    "for idx, row in errors.iterrows():\n",
    "    print(f'query: {row[\"query\"]}, \\n true_label: {row[\"true_label\"]}, pred_label: {row[\"predicted_label\"]}, confidence: {row[\"confidence\"]}, reasoning: {row[\"reasoning\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество предсказаний с уверенностью 1.0: 12\n",
      "Точность для уверенных предсказаний: 91.67%\n",
      "Полнота: 50.00%\n",
      "Точность: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Фильтруем результаты с confidence = 1.0\n",
    "high_conf_results = results_df[results_df['confidence'] > 0.95]\n",
    "\n",
    "# Считаем метрики\n",
    "correct = sum(high_conf_results['true_label'] == high_conf_results['predicted_label'])\n",
    "total = len(high_conf_results)\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "# Подсчет полноты и точности\n",
    "true_positives = sum((high_conf_results['true_label'] == 1) & (high_conf_results['predicted_label'] == 1))\n",
    "total_actual_positives = sum(high_conf_results['true_label'] == 1)\n",
    "total_predicted_positives = sum(high_conf_results['predicted_label'] == 1)\n",
    "\n",
    "recall = true_positives / total_actual_positives if total_actual_positives > 0 else 0\n",
    "precision = true_positives / total_predicted_positives if total_predicted_positives > 0 else 0\n",
    "\n",
    "print(f\"Количество предсказаний с уверенностью 1.0: {total}\")\n",
    "print(f\"Точность для уверенных предсказаний: {accuracy:.2%}\")\n",
    "print(f\"Полнота: {recall:.2%}\")\n",
    "print(f\"Точность: {precision:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WFSD-LP (107.9 FM) is a low-power FM radio sta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question asks for a specific detail (\"What...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Parliamentary Commissioner for Standards i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question explicitly asks for information (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rraboshtë is a village located in the former K...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question asks for specific details (\"what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The third century AD showed some remarkable de...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question asks for a specific detail from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nan Is the following statement true or false: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This is a question about a general, widely kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The history of the Marine Corps began when two...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question explicitly asks for information (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The 2009 L'Aquila earthquake occurred in the r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question explicitly asks for information (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Husinec is located about 6 kilometres (4 mi) n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question specifically asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>\"The Fox in the Attic\" was originally publishe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This question requires retrieving specific pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Rhodes Scholarships are international postgrad...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question explicitly asks for information (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Joseph Robinette Biden Jr. was born on Novembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question asks for specific details (\"maide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>nan Classify each of the numbers as prime or c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This task requires applying a specific mathema...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  true_label  \\\n",
       "1   WFSD-LP (107.9 FM) is a low-power FM radio sta...           0   \n",
       "5   The Parliamentary Commissioner for Standards i...           0   \n",
       "6   Rraboshtë is a village located in the former K...           0   \n",
       "17  The third century AD showed some remarkable de...           0   \n",
       "19  nan Is the following statement true or false: ...           1   \n",
       "21  The history of the Marine Corps began when two...           0   \n",
       "35  The 2009 L'Aquila earthquake occurred in the r...           0   \n",
       "42  Husinec is located about 6 kilometres (4 mi) n...           0   \n",
       "70  \"The Fox in the Attic\" was originally publishe...           0   \n",
       "73  Rhodes Scholarships are international postgrad...           0   \n",
       "83  Joseph Robinette Biden Jr. was born on Novembe...           0   \n",
       "96  nan Classify each of the numbers as prime or c...           1   \n",
       "\n",
       "    predicted_label  confidence  \\\n",
       "1                 0         1.0   \n",
       "5                 0         1.0   \n",
       "6                 0         1.0   \n",
       "17                0         1.0   \n",
       "19                1         1.0   \n",
       "21                0         1.0   \n",
       "35                0         1.0   \n",
       "42                0         1.0   \n",
       "70                0         1.0   \n",
       "73                0         1.0   \n",
       "83                0         1.0   \n",
       "96                0         1.0   \n",
       "\n",
       "                                            reasoning  \n",
       "1   The question asks for a specific detail (\"What...  \n",
       "5   The question explicitly asks for information (...  \n",
       "6   The question asks for specific details (\"what ...  \n",
       "17  The question asks for a specific detail from t...  \n",
       "19  This is a question about a general, widely kno...  \n",
       "21  The question explicitly asks for information (...  \n",
       "35  The question explicitly asks for information (...  \n",
       "42  The question specifically asks for information...  \n",
       "70  This question requires retrieving specific pub...  \n",
       "73  The question explicitly asks for information (...  \n",
       "83  The question asks for specific details (\"maide...  \n",
       "96  This task requires applying a specific mathema...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_conf_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
